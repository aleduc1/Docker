Get Docker :
~ Overview
	- Docker available in 2 edition : CE and EE
	- CE is for everyone community edition
	- EE is for enterprise and IT teams

~ About docker CE
	- 3 updates channel
	- Stable : latest/regular
	- Test : Testing..
	- Nightly : WIP for next major release/ Its in favor of CE Edge
	- Update are available as package on docker.com

~ Docker for mac
	- Hyperkit replace VB as a native VM system for mac
	- It doesnt have to stop the machine you has set up on VB with docker-machine, both can coexist (see later)

~ Toolbox
	- Toolbox does what we do manually at 42, its the other way to set up a docker environnement for mac
	- It installs :
		- Docker Engine (+ Docker CLI)
		- Docker machine
		- Docker compose
		- VirtualBox
		- Kitematic
	- Docker engine is the docker daemon on your computer
	But its doesnt run natively except on linux
	So docker-machine can lauch a VM via VirtualBox preconfigured to boot docker engine on it
	Then by settings ENV_VARIABLES for docker in your own shell you'll be able to use docker CLI from your terminal to communicate with the docker daemon running on Linux

~ Kitematic
	- It simplify and does the installation mentionned above for you
	- It provide an easily understandable GUI interface for average users


Get Started :
~ Overview
	- Docker enables you to separate your applications from your infrastructure so you can deliver software anywhere quickly
	- Docker Engine install the Docker Daemon as well as the Docker CLI and the REST API on your environnement
	- Docker CLI, Docker-machine, Docker-compose and kitematic communicate to the Docker Daemon through the REST API
	- Docker Daemon interact with the kernel to perform tasks
	- Docker Daemon manages Docker objects, such as images, containers, networks, and volumes.
	- Docker Hub, Cloud, and Store are the place where you can pull, or push Docker images.

~ Overview of docker object
	Images :
		- An image is a read-only template
		- An image is an executable package that includes everything needed to run an application ->
		- It contains instructions for creating a Docker container
		- An image can be based on another image with additionnal customisation
		- To build your own image you create a dockerfile with set of instructions following a correct syntax for those type of files
	Containers :
		- A container is a runnable/runtime instance of an image (I.e What the image become in memory when executed)
		- You can connect container to network
		- You can create a new image based on the current container state
		- When you're done with the executable/application inside this container, the container wont be deleted if non-specified, you can start it again for example
	Services :
		- Services allow you to scale container accross multiple Docker daemons working together as a swarm with multiple manager and worker
		- Each machine in the swarm is now called a node, (I.e. Worker node/ Manager node)

~ Part 1
	- Just setting up and basic commands

~ Part 2
	- Dockerfile defines what goes on in the environment container, inside you also often expose port 80 wich allow http output visualisation for the outside world
	- Once dockerfile is done, you should be in directory with dockerfile, requirment.txt and app.py for a python app
	- Then you can build the image

	- Use docker build . or docker build -t friendlyname .
	- Then if you look at you're local docker image registry with docker image ls
	- It should show your friendlyname image built
	- Then to run the app you have to map your machine’s port 4000 to the container’s published port 80 using -p like so :
	- docker run -p 4000:80 friendlyname
	- http://localhost:4000 on a web browser to see the output of app.py or curl http://localhost:4000 in a shell
	- docker run -d -p 4000:80 friendlyhello to run in background/detached mode
	- docker container ls to check if its still running
	- docker container stop with the id to stop it

	- Now to test portability, try running it from somewhere else. For that you need to Share the image
	- Create account at hub.docker.com and then docker login to connect
	- username/repository:tag is the notation to tag an image
	- docker tag friendlyname ywoak/get-started:part2
	- docker image ls to see it
	- then docker push username/repository:tag to push it

	- Now on any machine : docker run -p 4000:80 ywoak/get-started:part2 and it will run the image as a container and execute itself

~ Part 3
	- A docker-compose.yml file is a YAML file that define how container behave in production
	- It can pull an image, run numerous instance of that image so multiple containers at the same time
	- Limiting %of CPU usage by the containers
	- Put a restart policy in one fail to restart it, here its on-failure
	- Instruct all the containers running to share a port
	- Then with docker stack deploy -c docker-compose.yml nameoftheservice you created a network as instructed by the docker-compose file + the service itself
	- if you run docker ps you can see there is numerous container working on the same port
	- You now run a multi container machine

~ Part 4
	- A swarm is a group of machine all running docker joined into a cluster
	- Each machine is called a node
	- The docker command are now executed by a swarm manager wich tell wich or wich machine run the container
	- Strategie of swarm manager are instructed in a compose file
	- Only swarm manager node will execute the command
	- Worker node dont have authority to tell other machine what it can and cannot it
	- There are here solely for capacity

	- Enabling swarm mode instantly make your machine a swarm manager
	- It is done with docker swarm init
	- The node can be either physical or virtual machine
	- docker swarm join on other machine to make them join the swarm as worker

	- With docker-machine and the VB hypervisor you can create two VM running docker (I.e docker-machine create --driver virtualbox myvm1 and same with myvm2)
	- You can send command to VM using docker-machine ssh
	- So to instruct myvm1 to become a swarm manager with docker-machine ssh myvm1 "docker swarm init --advertise-addr <myvm1 ip>"
	- Always run docker swarm command with port 2377  wich is the swarm managment port and dont use port 2376 wich is the Docker daemon port
	- docker-machine ssh myvm2 "docker swarm join \ --token <token> \ <ip>:2377" and now you have created a swarm with a cluster of 2 machine running docker, wich are both virtual
	- Now run docker node ls on the manager to view the nodes in this swarm
	- Docker swarm leave on each node to start over

	- Instead of using ssh to talk to myvm1 you can configure your shell to speak directly through the myvm1 docker daemon using your host docker CLI
	- docker-machine env myvm1 and then eval ...
	- docker-machine ls again and now myvm1 should be the one active one
	- This is good so your compose file can run "remotely" kinda
	- To deplay the app on the swarm cluster repeat process seen in Part 3 in a swarm manager node
	- docker stack ps <nameoftheservice> to see now container are distributed in vm1 and vm2 compared to in part 3
	- Then you can do everything you learned in part 2 and 3 and stack deploy again
	- eval $(docker-machine env -u) to unset DOCKER_ENV_VARIABLES in your shell

~ Part 5
	- A stack is a group of inter-related services that share dependecies running together
	- They can be scaled together through stack
	- Since now we have been using a stack to create our service, but it was a single service stack

	- Just add another service in the compose file to add one then re run
	- Docker stack deploy -c docker-compose.yml <name of stack>

	- placement constraint can be specified in the docker-compose file to order wich node must run the program (I.e Manager or Worker).
	- It ensure that the app will keep the same filesystem throughout its use
	- Volume can be used to persist data. It act like a socket/SymLink between a file in the container and a file in the host
	- So if the container is torn-down and re deployed it'll keep the data in your host filesystem where you specified it

~ Part 6
	- Full cloud, we'll see later if needed

~ Develop with Docker

~ App development overview
	- Good dockerfile
	- Use multistage builds
	- Use volume to manage app data
	- Scale your app as a swarm service
	- Use compose file to define your stack

~ App development best practices
	- Keep image small :
		- Appropriate base image
		- Use multistage build
		- Use less RUN command and use && in one RUN command from dockerfile
		- Precise tag and do not rely on automatically-created latest tag
	- Persist application data :
		- Avoid storage drivers
		- Use volumes
		- Use secrets to store sensible data used by service
		- Else use configs
	- Use swarm whenever possible :
		- swarm service over standalone container
		- Network and volumes can be disconnected from the service and docker handle redeploying the service container in a non-disrupted way
		- Storing secrets and config are only available for services. And services run best in a swarm
	- Make good makefiles
		- Will see later in dockerfile docs

~ Configure networking
	- Networking overview :
		- They are 5 drivers you can use for core networking functionnality
			- Bridge : The default network driver. Use for standalone container who need to communicate
			- Host : Use for standalone container if you wanna remove networking isolation between a container and its host, and use the host network directly
			- Overlay : Enable swarm service to communicate with each other + Connect multiple daemon docker together
			- Macvlan : Allow you to assign a MAC adress on a container. Making it appear as a physical device on your network
			- none : For this container disable all networking
	- The rest of the doc is explanation + example for each type. Check if needed

~ Manage application data
	- Storage overview :
		- By default all files created inside a container are stored on the writable container layer.
		- So data doesnt persist at the end of the container runtime and its hard to move the data elsewhere.
		- Docker has two options for containers to store files in the host machine, so that the data are persisted even after the container stops: volumes and bind mounts. (tmpfs mount if you're on Linux.)
		- Volumes are created and managed by docker and isolated from the core functionnality of the host machine
		- Volumes also support volume drivers that allows to store data on remote hosts or cloud providers for example

		- Bind mounts is a file or a directory in the host machine that is mount to the container, it is referenced by its full path in the host.
		- It is created on demand if it doesnt exist yet
		- It relies on the host machine filesystem structure
		- Docker CLI commands cant directly manage bind mounts compared to Volumes

~ Dockerfile reference :
	- A dockerfile is a text document that contain all the command a user could call on the command line to assemble an image
	- Dockerfile contains instruction for docker to build an image
~ Usage
	- Docker build use dockerfile and build context
	- Build context is either path or url of a github rep
	- It includes subdirectories because it process recursively
	- Usually keep a directory empty and only put dockerfile and whats needed for the dockerfile to build the image
	- Dont use / as its recursive
	- -f to specify path/to/a/Dockerfile when docker build
	- -t to specify a repo and tag to save the image when build finish
~ Format
	- # Comment
	  INSTRUCTION arguments
	- Instruction isnt case sensitive but its convention to CAPS them
	- Docker run instructions from the dockerfile in order
	- Dockerfile must start with a FROM instruction wich specify the base image from wich you're building
~ Environment remplacement
	- Env variable are declared with the ENV instruction
	- """""""""""" can be used in certains instructions as variables to be interpreted by the dockerfile
	- Syntax are $VAR or ${VAR}
	- ${variable:-word} indicates that if variable is set then the result will be that value. If variable is not set then word will be the result.
	- ${variable:+word} indicates that if variable is set then word will be the result, otherwise the result is the empty string.
	- Env var are supported by following list of instructions in the dockerfile
	- ADD / COPY / ENV / EXPOSE / FROM / LABEL / STOPSIGNAL / USER / VOLUME / WORKDIR / ONBUILD when combine with others
~ .dockerignore file
	- .dockerignore file are interpreted by the docker CLI before sending the build context to the docker daemon. This way we can modify the build context to avoid sending huge useless PATH
	- /foo/bar and foo/bar is the same, because in a dockerignore the root is considered the working directory
	- docker CLI interpret the dockerignore file as newline separated for interpretation
	- # is a comment
	- */temp* exclude files and directories whose names start with temp in any immediate subdirectory of the root
	- */*/temp* exclude files and directories starting with temp from any subdirectory that is two levels below the root
	- temp? exclude files and directories in the root directory whose names are a one-character extension of temp. For example, /tempa and /tempb are excluded.
	- Matching is done using Go's filepath.Match rules but docker support ** that matches any number of directories
	- **/*.go will exclude all files that end with .go that are found in all directories, including the root of the build context.
	- Line starting with ! specify exception to exclusion like *md \n !README.md
	- You can excluse everything with * and then next line doing exception to specify wich one to include in the build context
	- You can exclude Dockerfile and .dockerignore, the daemon will still get them but ADD and CPY will not cpy them to the image
~ FROM
	- The FROM instruction initialize a new build stage and set the base image for the rest of the instructions
	- FROM <image> [AS <name>] AS name can be given for future referencing of the image build in this layer/stage
	- You can also specify tag, by default it try to pulls the latest tag of the image
	- FROM instructions support variables that are declared by any ARG instructions that occur before the first FROM
~ RUN
	- RUN has 2 forms : exec and shell forms
	- RUN <command> : shell form = the command is run in a shell, by default it is /bin/sh -c on Linux
	- RUN ["executable", "param1", "param2"] : self explanatory
	- The RUN instruction will create a new layer to do the command. The newly commited image will be used for the rest of the Dockerfile
	- \ can continue a single RUN app. Allow to be more clear
	- To use different shell use the exec form
	- Exec form doesnt invoke a command shell by default
	- RUN ["/bin/bash", "-c", "echo hello"] to change shell and specify what to do
	- The exec form is parsed as a JSON array, so u need double quotes and escaping \ with \
~ CMD
	- The CMD instruction has three forms:
	- CMD ["executable","param1","param2"] (exec form, this is the preferred form)
	- CMD ["param1","param2"] (as default parameters to ENTRYPOINT)
	- CMD command param1 param2 (shell form)
	- There can only be one CMD instructions in a Dockerfile : The last one
	- The main purpose of a CMD is to provide defaults for an executing container. These defaults can include an executable, or they can omit the executable, in which case you must specify an ENTRYPOINT instruction as well.
	- Note: If CMD is used to provide default arguments for the ENTRYPOINT instruction, both the CMD and ENTRYPOINT instructions should be specified with the JSON array format.
	- If you would like your container to run the same executable every time, then you should consider using ENTRYPOINT in combination with CMD
~ LABEL
	- The LABEL instruction adds metadata to an image
	- A LABEL is a key-value pair
	- To include spaces within a LABEL value, use quotes and backslashes as you would in command-line parsing
	- An image can have multiple label in the same LABEL instructions either with \ or just space in between them
~ MAINTAINER
	- Deprecated
~ EXPOSE
	- EXPOSE <port> [<port>/<protocol>...]
	- The EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime
	- You can specify whether the port listens on TCP or UDP, and the default is TCP if the protocol is not specified
	- The EXPOSE instruction does not actually publish the port, it informs the user when running docker run -p wich port to publish and map
	- To expose on both TCP and UDP, include two lines:
	- EXPOSE 80/tcp
	- EXPOSE 80/udp
~ ENV
	- ENV <key> <value>
	- ENV <key>=<value> ...
	- The ENV instruction sets the environment variable <key> to the value <value>
	- First syntax set one variable per instructions
	- 2nd can set multiple at once wich escaping space or quoting them
	- To set a value for a single command, use RUN <key>=<value> <command>
~ ADD
	- ADD has two forms:
	- ADD [--chown=<user>:<group>] <src>... <dest>
	- ADD [--chown=<user>:<group>] ["<src>",... "<dest>"] (this form is required for paths containing whitespace)
	- --chown feature only works for Linux OS-based container
	- The ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the image at the path <dest>
	- Each <src> may contain wildcards and matching will be done using Go’s filepath.Match rules
	- The <dest> is an absolute path, or a path relative to WORKDIR, into which the source will be copied inside the destination container
	- All new files and directories are created with a UID and GID of 0, unless the optional --chown flag specifies a given username, groupname, or UID/GID combination to request specific ownership of the content added
	- Check doc for syntax for --chown if needed
	- The <src> path must be inside the context of the build
	- If <src> is a URL and <dest> does not end with a trailing slash, then a file is downloaded from the URL and copied to <dest>
	- If <src> is a URL and <dest> does end with a trailing slash, then the filename is inferred from the URL and the file is downloaded to <dest>/<filename>
	- If <src> is a directory, the entire contents of the directory are copied, including filesystem metadata
	- If <src> is a local tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory. Resources from remote URLs are not decompressed
	- If <src> is any other kind of file, it is copied individually along with its metadata. In this case, if <dest> ends with a trailing slash /, it will be considered a directory and the contents of <src> will be written at <dest>/base(<src>)
	- If multiple <src> resources are specified, either directly or due to the use of a wildcard, then <dest> must be a directory, and it must end with a slash /
	- If <dest> does not end with a trailing slash, it will be considered a regular file and the contents of <src> will be written at <dest>
	- If <dest> doesn’t exist, it is created along with all missing directories in its path
~ COPY
	- COPY has two forms:
	- COPY [--chown=<user>:<group>] <src>... <dest>
	- COPY [--chown=<user>:<group>] ["<src>",... "<dest>"] (this form is required for paths containing whitespace)
	- The COPY instruction copies new files or directories from <src> and adds them to the filesystem of the container at the path <dest>
	- The rest is almost identical to ADD rules and features
~ ENTRYPOINT
	- ENTRYPOINT has two forms:
	- ENTRYPOINT ["executable", "param1", "param2"] (exec form, preferred)
	- ENTRYPOINT command param1 param2 (shell form)
	- An ENTRYPOINT allows you to configure a container that will run as an executable
	- Only the last ENTRYPOINT instruction in the Dockerfile will have an effect
	- You can use the exec form of ENTRYPOINT to set fairly stable default commands and arguments and then use either form of CMD to set additional defaults that are more likely to be changed.
	- FROM ubuntu
	- ENTRYPOINT ["top", "-b"]
	- CMD ["-c"]
	- Then if you look whats running in the container you'll see top -b -H if you said top -H during docker run command, it overriden CMD instruction but were happened after ENTRYPOINT
	- To ensure that docker stop will signal any long running ENTRYPOINT executable correctly, you need to remember to start it with exec (if you using ENTRYPOINT form shell)
	- Both CMD and ENTRYPOINT instructions define what command gets executed when running a container. There are few rules that describe their co-operation.
	- Dockerfile should specify at least one of CMD or ENTRYPOINT commands.
	- ENTRYPOINT should be defined when using the container as an executable.
	- CMD should be used as a way of defining default arguments for an ENTRYPOINT command or for executing an ad-hoc command in a container.
	- CMD will be overridden when running the container with alternative arguments.
	- Graph on their co-operation in the doc if needed
~ VOLUMES
	- The VOLUME instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers
	- If any build steps change the data within the volume after it has been declared, those changes will be discarded
	- The VOLUME instruction does not support specifying a host-dir parameter. You must specify the mountpoint when you create or run the container
~ USER
	- USER <user>[:<group>] or
	- USER <UID>[:<GID>]
	- The USER instruction sets the user name (or UID) and optionally the user group (or GID) to use when running the image and for any RUN, CMD and ENTRYPOINT instructions that follow it in the Dockerfile
	- When the user doesn’t have a primary group then the image (or the next instructions) will be run with the root group.
~ WORKDIR
	- WORKDIR /path/to/workdir 
	- The WORKDIR instruction sets the working directory for any instructions that follows it in the dockerfile
	- If the WORKDIR doesn’t exist, it will be created
~ ARG
	- ARG <name>[=<default value>]
	- An ARG instruction can optionally include a default value
	- If an ARG instruction has a default value and if there is no value passed at build-time, the builder uses the default
	- Environment variables defined using the ENV instruction always override an ARG instruction of the same name
	- Docker has a set of predefined ARG variables that you can use without a corresponding ARG instruction in the Dockerfile
	- HTTP_PROXY http_proxy HTTPS_PROXY https_proxy FTP_PROXY ftp_proxy NO_PROXY no_proxy
	- To use these, simply pass them on the command line using the flag : --build-arg <varname>=<value>
~ ONBUILD
	- ONBUILD [INSTRUCTION]
	- The ONBUILD instruction adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build
	- The trigger will be executed in the context of the downstream build, as if it had been inserted immediately after the FROM instruction in the downstream Dockerfile
	- When it encounters an ONBUILD instruction, the builder adds a trigger to the metadata of the image being built. The instruction does not otherwise affect the current build
	- At the end of the build, a list of all triggers is stored in the image manifest, under the key OnBuild. They can be inspected with the docker inspect command
	- Later the image may be used as a base for a new build, using the FROM instruction. As part of processing the FROM instruction, the downstream builder looks for ONBUILD triggers, and executes them in the same order they were registered
	- ONBUILD cant be chained with ONBUILD nor FROM
~ STOPSIGNAL
	- STOPSIGNAL signal
	- The STOPSIGNAL instruction sets the system call signal that will be sent to the container to exit
~ HEALTHCHECK
	- Useless for now, refer to doc later if needed
	- The HEALTHCHECK instruction tells Docker how to test a container to check that it is still working
~ SHELL
	- SHELL ["executable", "parameters"]
	- The SHELL instruction allows the default shell used for the shell form of commands to be overridden
	- Each SHELL instruction overrides all previous SHELL instructions
	- Mostly used for CMD ENTRYPOINT and RUN

~ BEST PRACTICE FOR WRITING DOCKERFILES :
	- EACH INSTRUCTION OF A DOCKERFILE IS A LAYER FOR THE DOCKER IMAGE
	- When you run an image and generate a container you add a new layer on top of the layer stack, its called the container layer
	- Any changes made after are written on this container layer, thats what allow to build an image based on the current container
	- -f for build context if dockerfile and file to act on are on different file
	- Use multi-stage build
	- Minimize the number of layers
	- space and \ if needed to add multiple arguments make it easier to read, for examples git install -y \arg1 \arg2 ...
	- Leverage build cache
